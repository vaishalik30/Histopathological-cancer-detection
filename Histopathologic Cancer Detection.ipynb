{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf507007618542bdc0f4aca885e5855d0dec7e18"
   },
   "source": [
    "# **Histopathologic Cancer Detection**\n",
    "***Identification of Metastatic Tissue in Histopathologic Scans of Lymph Node Sections***    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9891a3f0b3dc9d4e53042330e57f44eeae6b1685"
   },
   "source": [
    "**The images are labeled as 0 or 1, where 0 = No Tumor Tissue and 1 = Has Tumor Tissue(s)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e37caaed3fcf7ce6022845347d7ff01eb7550f3f"
   },
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Total Samples Available\n",
    "print(len(os.listdir('../input/train')))\n",
    "print(len(os.listdir('../input/test')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ea02846b55222461342cec080b44f1261804db4"
   },
   "source": [
    "### Creatig a DataFrame of Training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b862721d2db8b1aa633e19ae38cc30f8dc62d1d7"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/train_labels.csv')\n",
    "print('Shape of DataFrame',df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ddfbe3be86859a68acb71b0cc631fe5702552c1"
   },
   "source": [
    "### Deleting 2 images as they caused error in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "adab435e9f6481bcd2009f847f64e89eab0b44fd"
   },
   "outputs": [],
   "source": [
    "# removing this image because it caused a training error previously\n",
    "df[df['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n",
    "\n",
    "# removing this image because it's black\n",
    "df[df['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "406180d03beea5afc5675e72150b0395db4b0829"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (6,6)) \n",
    "ax = sns.countplot(df.label).set_title('Label Counts', fontsize = 18)\n",
    "plt.annotate(df.label.value_counts()[0],\n",
    "            xy = (0,df.label.value_counts()[0] + 2000),\n",
    "            va = 'bottom',\n",
    "            ha = 'center',\n",
    "            fontsize = 12)\n",
    "plt.annotate(df.label.value_counts()[1],\n",
    "            xy = (1,df.label.value_counts()[1] + 2000),\n",
    "            va = 'bottom',\n",
    "            ha = 'center',\n",
    "            fontsize = 12)\n",
    "plt.ylim(0,150000)\n",
    "plt.ylabel('Count', fontsize = 16)\n",
    "plt.xlabel('Labels', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf872d7e100c23dc7256c88eca0353b754d56e8a"
   },
   "source": [
    "There is a little imbalance in the lables which needs be rectified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a28db6e854cf3345e28393ca4a42738178207555"
   },
   "source": [
    "# **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6cbe264f9225f0613a6587c3d8adcd9e3854cda"
   },
   "source": [
    "### **Take 80K images from both categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "67cb329de44b201e5190b81bc914669c80ac303a"
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 80000\n",
    "\n",
    "df_negative = df[df['label'] == 0].sample(SAMPLE_SIZE, random_state = 0)\n",
    "\n",
    "df_positive = df[df['label'] == 1].sample(SAMPLE_SIZE, random_state = 0)\n",
    "\n",
    "\n",
    "df_train = pd.concat([df_negative, df_positive], axis = 0).reset_index(drop = True)\n",
    "\n",
    "df_train = shuffle(df_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89d02968ae213d657d0fffcf548248932a8e023e"
   },
   "source": [
    "### **Spliting the shuffled images into training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e303dcd33773b33e43e553ca831da31f6f7be263"
   },
   "outputs": [],
   "source": [
    "\n",
    "y = df_train['label']\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size = 0.1, random_state = 0, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69bbae64fa38dcc9961b0a545eb4cc9d3dbcbc9f"
   },
   "source": [
    "**Creating Directory Structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c47212e910f096dccb21e6603686218253c0b2d"
   },
   "outputs": [],
   "source": [
    "\n",
    "basedirectory = 'basedirectory'\n",
    "os.mkdir(basedirectory)\n",
    "\n",
    "\n",
    "training_directory = os.path.join(basedirectory, 'training_directory')\n",
    "os.mkdir(training_directory)\n",
    "\n",
    "\n",
    "validation_directory = os.path.join(basedirectory, 'validation_directory')\n",
    "os.mkdir(validation_directory)\n",
    "\n",
    "\n",
    "no_tumor = os.path.join(training_directory, '0')\n",
    "os.mkdir(no_tumor)\n",
    "has_tumor = os.path.join(training_directory, '1')\n",
    "os.mkdir(has_tumor)\n",
    "\n",
    "no_tumor = os.path.join(validation_directory, '0')\n",
    "os.mkdir(no_tumor)\n",
    "has_tumor = os.path.join(validation_directory, '1')\n",
    "os.mkdir(has_tumor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d26cfa79a1373d0eac3ee1f13a74f5b11b93df1"
   },
   "source": [
    "**Transfer the respective images into their respective folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8ca025d9524110a511941f81bffc6a689426589"
   },
   "outputs": [],
   "source": [
    "df.set_index('id', inplace=True)\n",
    "\n",
    "traininglist = list(df_train['id'])\n",
    "validationlist = list(df_val['id'])\n",
    "\n",
    "for image in traininglist:\n",
    "    imagename = image + '.tif'\n",
    "    target = df.loc[image,'label']\n",
    "    if target == 0:\n",
    "        label = '0'\n",
    "    elif target == 1:\n",
    "        label = '1'\n",
    "    \n",
    "    src = os.path.join('../input/train', imagename)\n",
    "    dest = os.path.join(train_dir, label, imagename)\n",
    "    shutil.copyfile(src, dest)\n",
    "\n",
    "for image in validationlist:  \n",
    "    imagename = image + '.tif'\n",
    "    target = df.loc[image,'label']\n",
    "    if target == 0:\n",
    "        label = '0'\n",
    "    elif target == 1:\n",
    "        label = '1'\n",
    "    src = os.path.join('../input/train', imagename)\n",
    "    dest = os.path.join(val_dir, label, imagename)\n",
    "    shutil.copyfile(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79b25f0ddbd2b0e25ae068121adee9babaac24aa"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "IMAGE_SIZE = 96\n",
    "training_path = 'base_dir/train_dir'\n",
    "validation_path = 'base_dir/val_dir'\n",
    "testing_path = '../input/test'\n",
    "training_sample_count = len(df_train)\n",
    "validation_sample_count = len(df_val)\n",
    "training_batchsize = 32 #10\n",
    "validation_batchsize = 32 #10\n",
    "\n",
    "train_steps = np.ceil(training_sample_count / training_batchsize)\n",
    "val_steps = np.ceil(validation_sample_count / validation_batchsize)\n",
    "datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(training_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=training_batchsize,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "val_gen = datagen.flow_from_directory(validation_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=validation_batchsize,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "test_gen = datagen.flow_from_directory(validation_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "41dc501ceb24e5b1bdd25c745fd9318267959e7e"
   },
   "source": [
    "# **Model Creation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "460dc2303b867c65d8597cd90fab142ddbe340e5"
   },
   "outputs": [],
   "source": [
    "#Import Keras\n",
    "import keras\n",
    "import tensorflow.keras\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import SeparableConv2D\n",
    "from keras.layers.core import Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "04bec957f0d95b3effae51c16a989b507ff687d6"
   },
   "outputs": [],
   "source": [
    "class CNNNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "            inputShape = (height, width, depth)\n",
    "            model = Sequential()\n",
    "            \n",
    "            model.add(Conv2D(filters = 32, kernel_size = (5,5), padding=\"same\", activation='relu', input_shape= inputShape))\n",
    "            model.add(Conv2D(filters = 32, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(Conv2D(filters = 32, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Dropout(0.2))\n",
    "                      \n",
    "            model.add(Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Dropout(0.2))\n",
    "            \n",
    "            model.add(Conv2D(filters = 128, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(Conv2D(filters = 128, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(Conv2D(filters = 128, kernel_size = (3,3), padding=\"same\", activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Dropout(0.25))\n",
    "            \n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(units = 500, activation = 'relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "\n",
    "            model.add(Dense(classes, activation='softmax'))   \n",
    "            model.summary() \n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e0dffe24441e9c2916b0963f545a2f06dce484b"
   },
   "outputs": [],
   "source": [
    "class CancerCNN:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        model.add(SeparableConv2D(32, (3, 3), padding=\"same\",input_shape = inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))        \n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76501728ae07bbfadf444856cb960415b0d1446b"
   },
   "outputs": [],
   "source": [
    "model = CNNNet.build(width = 96, height = 96, depth = 3, classes = 2)\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adagrad\n",
    "model.compile(optimizer = Adam(lr=0.0001), loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e73437b74bbe2883abe87b3e43cff8c3d962b56b"
   },
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "956b166c16f529c98512e3ee429980ad7e6271b6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "filepath = \"saved_model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose = 1, \n",
    "                             save_best_only = True, mode = 'max') \n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor = 0.5, patience = 2, verbose = 1, mode = 'max', min_lr = 0.00001)                              \n",
    "callbacks_list = [checkpoint, reduce_lr] \n",
    "\n",
    "history = model.fit_generator(train_gen, steps_per_epoch = train_steps, \n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = val_steps,\n",
    "                    epochs = 8,\n",
    "                    verbose = 1,\n",
    "                    callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bbdf26a8e7e8dadda170032e835214200138374a"
   },
   "source": [
    "# **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "830f161dbc3ed7b78f2f77b2c7502e8f60c48ffa"
   },
   "source": [
    "### **Compare Training and Validation Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94ed027e01cbea6fcadbb7e26f94316302146b98"
   },
   "source": [
    "We can determine our epochs based on the convergence of below graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98303020b3956a521d482e5faec939247a25e7db"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab9762944fd603c2e6397f386733aca8828d096b"
   },
   "source": [
    "### **Load the saved weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b87f923b7e9b77a2fe10012f7d63fb68636363b"
   },
   "outputs": [],
   "source": [
    "# Here the best epoch will be used.\n",
    "model.load_weights('saved_model.h5')\n",
    "val_loss, val_acc = \\\n",
    "model.evaluate_generator(test_gen, steps=len(df_val))\n",
    "print('val_loss:', val_loss)\n",
    "print('val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3fa0a7472c5c7cd4bdf1882d4c6798e7524e0e39"
   },
   "source": [
    "### **Validate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e486814e4c1ed4f8777e4f5efa5c1487056ab63"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f32e41a752dcb071ba2b4dc79810d5bc7f105fd7"
   },
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame(predictions, columns=['no_tumor', 'has_tumor'])\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4fc00f43fbf7473a8ac0abe3142f3ea36f0bb3fe"
   },
   "outputs": [],
   "source": [
    "y_true = test_gen.classes\n",
    "y_pred = df_preds['has_tumor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84f67c255101f1031aff34e852a163f5e97a3b1a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "print('ROC AUC Score = ',roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cda221a6c8ff013907be72c92f318708f3895ad9"
   },
   "outputs": [],
   "source": [
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_true, y_pred)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b228c0089bd4d23e1efcabee3745faad3a8d0870"
   },
   "source": [
    "**Let's plot our ROC Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ab8c5f0d97d592895d645d1896e496c880740bc"
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='area = {:.2f}'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d460346ca9dcb69d12636a1e41a721e13803117a"
   },
   "source": [
    "## **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62dd86efa45a118cf559c8ed2f0f1bc9a3ccdb11"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_binary = predictions.argmax(axis=1)\n",
    "cm = confusion_matrix(y_true, y_pred_binary)\n",
    "\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,\n",
    "                               cmap = 'Dark2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "945a87b858b89305d3eeca6a75d4312eb9559cde"
   },
   "source": [
    "## **Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c600c3454c4d21890d9725e7c98b1d51e2f316d4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Generate a classification report\n",
    "\n",
    "report = classification_report(y_true, y_pred_binary, target_names = ['no_tumor', 'has_tumor'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "529cca23e283883a3b66dee8ba41315e6c587554"
   },
   "outputs": [],
   "source": [
    "shutil.rmtree('base_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32947e3337e8df86109fb5f91fbab545fc37febd"
   },
   "source": [
    "# **Test Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "87323860d2cf7fdb76fa3a1f1eb37dd31deb6908"
   },
   "outputs": [],
   "source": [
    "test_dir = 'test_dir'\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "test_images = os.path.join(test_dir, 'test_images')\n",
    "os.mkdir(test_images)\n",
    "\n",
    "os.listdir('test_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5679e1bbead0a2dadfed9813b35b2b67c2fc98e3"
   },
   "outputs": [],
   "source": [
    "test_list = os.listdir('../input/test')\n",
    "\n",
    "for image in test_list:    \n",
    "    fname = image\n",
    "    src = os.path.join('../input/test', fname)\n",
    "    dst = os.path.join(test_images, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print('Total Test Images = ',len(os.listdir('test_dir/test_images')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aafa0c8c4448b1b7894f6a1363211bdd3fd80fc9"
   },
   "outputs": [],
   "source": [
    "test_path ='test_dir'\n",
    "test_gen = datagen.flow_from_directory(test_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "573552072125f0b853626484e60cc8c3454a4a8d"
   },
   "outputs": [],
   "source": [
    "num_test_images = 57458 \n",
    "predictions = model.predict_generator(test_gen, steps=num_test_images, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f02f0edef530aed42e2752fc13b07fa7ad3feae4"
   },
   "outputs": [],
   "source": [
    "if predictions.shape[0] == num_test_images:\n",
    "    print('All Predictions Done!')\n",
    "else:\n",
    "    print('Error!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0743dc8a0144451ce1f612ac456fd917d05b32e0"
   },
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame(predictions, columns=['no_tumor', 'has_tumor'])\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3521c8ad4f4870a29e3cef1afc9d791c354c3185"
   },
   "outputs": [],
   "source": [
    "test_filenames = test_gen.filenames\n",
    "\n",
    "df_preds['file_names'] = test_filenames\n",
    "\n",
    "def extract_id(x):\n",
    "    a = x.split('/')\n",
    "    b = a[1].split('.')\n",
    "    extracted_id = b[0]\n",
    "    return extracted_id\n",
    "df_preds['id'] = df_preds['file_names'].apply(extract_id)\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28f6cf69c8b269c675188259cea0614f02a838b1"
   },
   "outputs": [],
   "source": [
    "y_pred = df_preds['has_tumor']\n",
    "image_id = df_preds['id']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
